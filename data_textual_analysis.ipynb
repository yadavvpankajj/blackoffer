{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mryad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mryad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename                                               text\n",
      "0  blackassign0001.txt  Rising IT cities and its impact on the economy...\n",
      "1  blackassign0002.txt  Rising IT Cities and Their Impact on the Econo...\n",
      "2  blackassign0003.txt  Internet Demandâ€™s Evolution, Communication Imp...\n",
      "3  blackassign0004.txt  Rise of Cybercrime and its Effect in upcoming ...\n",
      "4  blackassign0005.txt  OTT platform and its impact on the entertainme...\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Failed to extract article for URL_ID text.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Failed to extract article for URL_ID text.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Article text analyzed successfully.\n",
      "Textual analysis completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "# Ensure required NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the input and output structure Excel files\n",
    "output_structure_path =\"C:\\\\Users\\\\mryad\\\\Downloads\\\\Output Data Structure (1).xlsx\"\n",
    "input_path=\"C:\\\\Users\\\\mryad\\\\Downloads\\\\Input (1).xlsx\"\n",
    "output_structure_df = pd.read_excel(output_structure_path)\n",
    "input_df=pd.read_excel(input_path)\n",
    "\n",
    "directory =\"C:\\\\Users\\\\mryad\\\\Downloads\\\\URL_ID\"\n",
    "def load_text_files(directory):\n",
    "    text_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text_data.append({'filename': filename, 'text': text})\n",
    "    return pd.DataFrame(text_data)\n",
    "# Load the text files into a DataFrame\n",
    "text_df = load_text_files(directory)\n",
    "print(text_df.head())\n",
    "# Define positive and negative word lists (these should be more comprehensive in a real use case)\n",
    "negative_words =\"C:\\\\Users\\\\mryad\\\\Downloads\\\\MasterDictionary-20240619T101032Z-001\\\\MasterDictionary\\\\negative-words.txt\"\n",
    "positive_words =\"C:\\\\Users\\\\mryad\\\\Downloads\\\\MasterDictionary-20240619T101032Z-001\\\\MasterDictionary\\\\positive-words.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if word.endswith(\"le\") and len(word) > 2 and word[-3] not in vowels:\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# Function to extract article content\n",
    "def extract_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract the article title\n",
    "            title_tag = soup.find('h1')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else 'No Title'\n",
    "            \n",
    "            # Extract the article text\n",
    "            paragraphs = soup.find_all('p')\n",
    "            article_text = \"\\n\".join([para.get_text(strip=True) for para in paragraphs])\n",
    "            \n",
    "            return title, article_text\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to perform textual analysis\n",
    "def analyze_text(text):\n",
    "    # Tokenize sentences and words\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Count positive and negative words\n",
    "    pos_count = sum(1 for word in words if word.lower() in positive_words)\n",
    "    neg_count = sum(1 for word in words if word.lower() in negative_words)\n",
    "    \n",
    "    # Calculate polarity and subjectivity\n",
    "    polarity_score = (pos_count - neg_count) / ((pos_count + neg_count) + 0.000001)\n",
    "    subjectivity_score = (pos_count + neg_count) / (num_words + 0.000001)\n",
    "    \n",
    "    # Calculate average sentence length\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    \n",
    "    # Count complex words\n",
    "    complex_words = [word for word in words if count_syllables(word) >= 3]\n",
    "    num_complex_words = len(complex_words)\n",
    "    \n",
    "    # Calculate percentage of complex words\n",
    "    perc_complex_words = (num_complex_words / num_words) * 100\n",
    "    \n",
    "    # Calculate fog index\n",
    "    fog_index = 0.4 * (avg_sentence_length + perc_complex_words)\n",
    "    \n",
    "    # Count syllables per word\n",
    "    total_syllables = sum(count_syllables(word) for word in words)\n",
    "    syllable_per_word = total_syllables / num_words\n",
    "    \n",
    "    # Count personal pronouns\n",
    "    personal_pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
    "    num_personal_pronouns = len(personal_pronouns)\n",
    "    \n",
    "    # Calculate average word length\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    avg_word_length = total_characters / num_words\n",
    "    \n",
    "    return {\n",
    "        'POSITIVE SCORE': pos_count,\n",
    "        'NEGATIVE SCORE': neg_count,\n",
    "        'POLARITY SCORE': polarity_score,\n",
    "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': perc_complex_words,\n",
    "        'FOG INDEX': fog_index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,\n",
    "        'COMPLEX WORD COUNT': num_complex_words,\n",
    "        'WORD COUNT': num_words,\n",
    "        'SYLLABLE PER WORD': syllable_per_word,\n",
    "        'PERSONAL PRONOUNS': num_personal_pronouns,\n",
    "        'AVG WORD LENGTH': avg_word_length\n",
    "    }\n",
    "\n",
    "# Create a list to store the results\n",
    "results_list = []\n",
    "\n",
    "# Iterate over each row in the input dataframe\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    title, article_text = extract_article(url)\n",
    "    if title and article_text:\n",
    "        content = f\"{title}\\n\\n{article_text}\"\n",
    "        \n",
    "        # Perform textual analysis\n",
    "        analysis_results = analyze_text(article_text)\n",
    "        \n",
    "        # Append the results to the list\n",
    "        results_list.append({\n",
    "            'URL_ID': url_id,\n",
    "            'Title': title,\n",
    "            'Article': article_text,\n",
    "            **analysis_results\n",
    "        })\n",
    "        \n",
    "        print(f\"Article {'text'} analyzed successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to extract article for URL_ID {'text'}.\")\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Ensure the results DataFrame matches the output structure\n",
    "for column in output_structure_df.columns:\n",
    "    if column not in results_df.columns:\n",
    "        results_df[column] = None\n",
    "\n",
    "# Reorder the DataFrame columns to match the output structure\n",
    "results_df = results_df[output_structure_df.columns]\n",
    "\n",
    "# Save the results to an Excel file\n",
    "output_file_path =\"C:\\\\Users\\\\mryad\\\\Downloads\\\\text_analysis_exel.xlsx\"\n",
    "results_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(\"Textual analysis completed and results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
